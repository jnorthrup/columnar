{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Welcome to this Kernel\n\n## * This kernel is a compilation of 75 exercises with solutions from this webpage:\n\nhttps://www.machinelearningplus.com/python/101-pandas-exercises-python/\n\n## <span style=\"color:green\">* If you want to learn **sklearn** check this kernel with tricks and tips:</span>\n\nhttps://www.kaggle.com/python10pm/sklearn-24-best-tips-and-tricks"},{"metadata":{},"cell_type":"markdown","source":"<a id='table_of_contents'></a>\n# Table of contents\n\n[1. How to import pandas and check the version?](#q1)\n\n[2. How to create a series from a list, numpy array and dict?](#q2)\n\n[3. How to convert the index of a series into a column of a dataframe?](#q3)\n\n[4. How to combine many series to form a dataframe?](#q4)\n\n[5. How to assign name to the series’ index?](#q5)\n\n[6. How to get the items of series A not present in series B?](#q6)\n\n[7. How to get the items not common to both series A and series B?](#q7)\n\n[8. How to get the minimum, 25th percentile, median, 75th, and max of a numeric series?](#q8)\n\n[9. How to get frequency counts of unique items of a series?](#q9)\n\n[10. How to keep only top 2 most frequent values as it is and replace everything else as ‘Other’?](#q10)\n\n[11. How to bin a numeric series to 10 groups of equal size?](#q11)\n\n[12. How to convert a numpy array to a dataframe of given shape?](#q12)\n\n[13. How to find the positions of numbers that are multiples of 3 from a series?](#q13)\n\n[14. How to extract items at given positions from a series?](#q14)\n\n[15. How to stack two series vertically and horizontally ?](#q15)\n\n[16. How to get the positions of items of series A in another series B?](#q16)\n\n[17. How to compute the mean squared error on a truth and predicted series?](#q17)\n\n[18. How to convert the first character of each element in a series to uppercase?](#q18)\n\n[19. How to calculate the number of characters in each word in a series?](#q19)\n\n[20. How to compute difference of differences between consequtive numbers of a series?](#q20)\n\n[21. How to convert a series of date-strings to a timeseries?](#q21)\n\n[22. How to get the day of month, week number, day of year and day of week from a series of date strings?](#q22)\n\n[23. How to convert year-month string to dates corresponding to the 4th day of the month?](#q23)\n\n[24. How to filter words that contain atleast 2 vowels from a series?](#q24)\n\n[25. How to filter valid emails from a series?](#q25)\n\n[26. How to get the mean of a series grouped by another series?](#q26)\n\n[27. How to compute the euclidean distance between two series?](#q27)\n\n[28. How to find all the local maxima (or peaks) in a numeric series?](#q28)\n\n[29. How to replace missing spaces in a string with the least frequent character?](#q29)\n\n[30. How to create a TimeSeries starting ‘2000-01-01’ and 10 weekends (saturdays) after that having random numbers as values?](#q30)\n\n[31. How to fill an intermittent time series so all missing dates show up with values of previous non-missing date?](#q31)\n\n[32. How to compute the autocorrelations of a numeric series?](#q32)\n\n[33. How to import only every nth row from a csv file to create a dataframe?](#q33)\n\n[34. How to change column values when importing csv to a dataframe?](#q34)\n\n[35. How to create a dataframe with rows as strides from a given series?](#q35)\n\n[36. How to import only specified columns from a csv file?](#q36)\n\n[37. How to get the nrows, ncolumns, datatype, summary stats of each column of a dataframe? Also get the array and list equivalent.](#q37)\n\n[38. How to extract the row and column number of a particular cell with given criterion?](#q38)\n\n[39. How to rename a specific columns in a dataframe?](#q39)\n\n[40. How to check if a dataframe has any missing values?](#q40)\n\n[41. How to count the number of missing values in each column?](#q41)\n\n[42. How to replace missing values of multiple numeric columns with the mean?](#q42)\n\n[43. How to use apply function on existing columns with global variables as additional arguments?](#q43)\n\n[44. How to select a specific column from a dataframe as a dataframe instead of a series?](#q44)\n\n[45. How to change the order of columns of a dataframe?](#q45)\n\n[46. How to set the number of rows and columns displayed in the output?](#q46)\n\n[47. How to format or suppress scientific notations in a pandas dataframe?](#q47)\n\n[48. How to format all the values in a dataframe as percentages?](#q48)\n\n[49. How to filter every nth row in a dataframe?](#q49)\n\n[50. How to create a primary key index by combining relevant columns?](#q50)\n\n[51. How to get the row number of the nth largest value in a column?](#q51)\n\n[52. How to find the position of the nth largest value greater than a given value?](#q52)\n\n[53. How to get the last n rows of a dataframe with row sum > 100?](#q53)\n\n[54. How to find and cap outliers from a series or dataframe column?](#q54)\n\n[55. How to reshape a dataframe to the largest possible square after removing the negative values?](#q55)\n\n[56. How to swap two rows of a dataframe?](#q56)\n\n[57. How to reverse the rows of a dataframe?](#q57)\n\n[58. How to create one-hot encodings of a categorical variable (dummy variables)?](#q58)\n\n[59. Which column contains the highest number of row-wise maximum values?](#q59)\n\n[60. How to create a new column that contains the row number of nearest column by euclidean distance?](#q60)\n\n[61. How to know the maximum possible correlation value of each column against other columns?](#q61)\n\n[62. How to create a column containing the minimum by maximum of each row?](#q62)\n\n[63. How to create a column that contains the penultimate value in each row?](#q63)\n\n[64. How to normalize all columns in a dataframe?](#q64)\n\n[65. How to compute the correlation of each row with the suceeding row?](#q65)\n\n[66. How to replace both the diagonals of dataframe with 0?](#q66)\n\n[67. How to get the particular group of a groupby dataframe by key?](#q67)\n\n[68. How to get the n’th largest value of a column when grouped by another column?](#q68)\n\n[69. How to compute grouped mean on pandas dataframe and keep the grouped column as another column (not index)?](#q69)\n\n[70. How to join two dataframes by 2 columns so they have only the common rows?](#q70)\n\n[71. How to remove rows from a dataframe that are present in another dataframe?](#q71)\n\n[72. How to get the positions where values of two columns match?](#q72)\n\n[73. How to create lags and leads of a column in a dataframe?](#q73)\n\n[74. How to get the frequency of unique values in the entire dataframe?](#q74)\n\n[75. How to split a text column into two separate columns?](#q75)\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Allow several prints in one cell\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pandas exercise"},{"metadata":{},"cell_type":"markdown","source":"<a id='q1'></a>\n**1. How to import pandas and check the version?**\n\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nprint(pd.__version__)\n\n# Print all pandas dependencies\nprint(pd.show_versions(as_json=True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q2'></a>\n\n**2. How to create a series from a list, numpy array and dict?**\n\nCreate a pandas series from each of the items below: a list, numpy and a dictionary\n\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Input\nimport numpy as np\na_list = list(\"abcdefg\")\nnumpy_array = np.arange(1, 10)\ndictionary = {\"A\":  0, \"B\":1, \"C\":2, \"D\":3, \"E\":5}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"series1 = pd.Series(a_list)\nprint(series1)\nseries2 = pd.Series(numpy_array)\nprint(series2)\nseries3 = pd.Series(dictionary)\nprint(series3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q3'></a>\n**3. How to convert the index of a series into a column of a dataframe?**\n\nConvert the series ser into a dataframe with its index as another column on the dataframe.\n\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\nmylist = list('abcedfghijklmnopqrstuvwxyz')\nmyarr = np.arange(26)\nmydict = dict(zip(mylist, myarr))\nser = pd.Series(mydict)\nprint(ser[:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# solution 1 using DataFrame\nser_df = pd.DataFrame(ser)\nser_df.reset_index()\n\n# using pandas to_frame()\nser_df = ser.to_frame().reset_index()\nser_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q4'></a>\n**4. How to combine many series to form a dataframe?**\n\nCombine ser1 and ser2 to form a dataframe.\n\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\nser1 = pd.Series(list('abcedfghijklmnopqrstuvwxyz'))\nser2 = pd.Series(np.arange(26))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using pandas DataFrame\nser_df = pd.DataFrame(ser1, ser2).reset_index()\nser_df.head()\n# using pandas DataFrame with a dictionary, gives a specific name to the column\nser_df = pd.DataFrame({\"col1\":ser1, \"col2\":ser2})\nser_df.head(5)\n# using pandas concat\nser_df = pd.concat([ser1, ser2], axis = 1)\nser_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q5'></a>\n**5. How to assign name to the series’ index?**\n\nGive a name to the series ser calling it ‘alphabets’.\n\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\nser = pd.Series(list('abcedfghijklmnopqrstuvwxyz'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using series rename method\nser.rename(\"alphabets\")\n# using series attribute\nser.name = \"other_name\"\nser","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q6'></a>\n**6. How to get the items of series A not present in series B?**\n\nGet all items of ser1 and ser2 not common to both.\n\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\nser1 = pd.Series([1, 2, 3, 4, 5])\nser2 = pd.Series([4, 5, 6, 7, 8])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ser1[~ser1.isin(ser2)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q7'></a>\n**7. How to get the items not common to both series A and series B?**\n\nGet all items of ser1 and ser2 not common to both.\n\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\nser1 = pd.Series([1, 2, 3, 4, 5])\nser2 = pd.Series([4, 5, 6, 7, 8])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using pandas\na_not_b = ser1[~ser1.isin(ser2)]\nb_not_a = ser2[~ser2.isin(ser1)]\n                          \na_not_b.append(b_not_a, ignore_index = True)\n\n# using numpy union and intersection\nser_u = pd.Series(np.union1d(ser1, ser2))\nser_i = pd.Series(np.intersect1d(ser1, ser2))\nser_u[~ser_u.isin(ser_i)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q8'></a>\n**8. How to get the minimum, 25th percentile, median, 75th, and max of a numeric series?**\n\nCompute the minimum, 25th percentile, median, 75th, and maximum of ser.\n\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\nstate = np.random.RandomState(100)\nser = pd.Series(state.normal(10, 5, 25))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using pandas\nser.describe()\n\n# or using numpy\nnp.percentile(ser, q = [0, 25, 50, 75, 100])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q9'></a>\n**9. How to get frequency counts of unique items of a series?**\n\nCalculate the frequency counts of each unique value ser.\n\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\nser = pd.Series(np.take(list('abcdefgh'), np.random.randint(8, size=30)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ser.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q10'></a>\n**10. How to keep only top 2 most frequent values as it is and replace everything else as ‘Other’?**\n\nFrom ser, keep the top 2 most frequent items as it is and replace everything else as ‘Other’.\n\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\nnp.random.RandomState(100)\nser = pd.Series(np.random.randint(1, 5, [12]))\nser","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ser.value_counts()\nser[~ser.isin(ser.value_counts().index[:2])] = 'Other'\nser\n# we do value_counts to see the repetitions for each value, then we do ~ser.isin value_counts, filter by index the first 2 and = \"Other renames the values\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q11'></a>\n**11. How to bin a numeric series to 10 groups of equal size?**\n\nBin the series ser into 10 equal deciles and replace the values with the bin name.\n\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\nser = pd.Series(np.random.random(20))\nser","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.qcut(ser, q = 10)\n# we can also pass labels\npd.qcut(ser, q = [0, .10, .20, .30, .40, .50, .60, .70, .80, .90, 1], labels=['1st', '2nd', '3rd', '4th', '5th', '6th', '7th', '8th', '9th', '10th']).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q12'></a>\n**12. How to convert a numpy array to a dataframe of given shape? (L1)**\n\nReshape the series ser into a dataframe with 7 rows and 5 columns\n\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\nser = pd.Series(np.random.randint(1, 10, 35))\nser","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using numpy\npd.DataFrame(np.array(ser).reshape(7, 5))\n\n# using only pandas\npd.DataFrame(ser.values.reshape(7, 5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q13'></a>\n**13. How to find the positions of numbers that are multiples of 3 from a series?**\n\nFind the positions of numbers that are multiples of 3 from ser.\n\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\n\nnp.random.RandomState(100)\nser = pd.Series(np.random.randint(1, 5, 10))\nser","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using the where clause\nser.where(lambda x: x%3 == 0).dropna()\n\n# using numpy and reshape to get a pandas series\n#pd.Series(np.argwhere(ser%3 == 0).reshape(4))\nnp.argwhere(ser%3 == 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q14'></a>\n**14. How to extract items at given positions from a series**\n\nFrom ser, extract the items at positions in list pos.\n\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\n\nser = pd.Series(list('abcdefghijklmnopqrstuvwxyz'))\npos = [0, 4, 8, 14, 20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using loc\nser.loc[pos]\n\n# using series take\nser.take(pos)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q15'></a>\n\n**15. How to stack two series vertically and horizontally ?**\n\nStack ser1 and ser2 vertically and horizontally (to form a dataframe).\n\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\nser1 = pd.Series(range(5))\nser2 = pd.Series(list('abcde'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# vertical\nser1.append(ser2)\n# or using pandas concat and axis = 0\npd.concat([ser1, ser2], axis = 0)\n\n# horizontal\npd.concat([ser1, ser2], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q16'></a>\n**16. How to get the positions of items of series A in another series B?**\n\nGet the positions of items of ser2 in ser1 as a list.\n\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\nser1 = pd.Series([10, 9, 6, 5, 3, 1, 12, 8, 13])\nser2 = pd.Series([1, 3, 10, 13])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get's the index, but it's sorts the index\nlist(ser1[ser1.isin(ser2)].index)\n\n# using numpy where\n[np.where(i == ser1)[0].tolist()[0] for i in ser2]\n\n# using pandas Index and get location\n[pd.Index(ser1).get_loc(i) for i in ser2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q17'></a>\n**17. How to compute the mean squared error on a truth and predicted series?**\n\nCompute the mean squared error of truth and pred series.\n\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\ntruth = pd.Series(range(10))\npred = pd.Series(range(10)) + np.random.random(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# BAD, don't use it\n(np.mean([(truth_i - pred_i)**2 for truth_i, pred_i in zip(truth, pred)]))\n\n# using numpy\nnp.mean((truth-pred)**2)\n\n# using sklear metrics\nfrom sklearn.metrics import mean_squared_error\nmean_squared_error(truth, pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q18'></a>\n\n**18. How to convert the first character of each element in a series to uppercase?**\n\nChange the first character of each word to upper case in each word of ser.\n\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\nser = pd.Series(['just', 'a', 'random', 'list'])\nser\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using python string method title() Assumes we only encounter string in the list\n[i.title() for i in ser]\n\n# using lambda\nser.map(lambda x: x.title())\n\n# other solution\nser.map(lambda x: x[0].upper() + x[1:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q19'></a>\n\n**19. How to calculate the number of characters in each word in a series?**\n\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\nser = pd.Series(['just', 'a', 'random', 'list'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using list comprehension\n[len(i) for i in ser]\n\n# using series map\nser.map(len)\n\n# using series apply\nser.apply(len)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q20'></a>\n**20. How to compute difference of differences between consequtive numbers of a series?**\n\nDifference of differences between the consequtive numbers of ser.\n\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\nser = pd.Series([1, 3, 6, 10, 15, 21, 27, 35])\n\n# Desired Output\n# [nan, 2.0, 3.0, 4.0, 5.0, 6.0, 6.0, 8.0]\n# [nan, nan, 1.0, 1.0, 1.0, 1.0, 0.0, 2.0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using pandas diff()\nser.diff(periods = 1).tolist()\nser.diff(periods = 1).diff(periods = 1).tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q21'></a>\n\n**21. How to convert a series of date-strings to a timeseries?**\n\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\nser = pd.Series(['01 Jan 2010', '02-02-2011', '20120303', '2013/04/04', '2014-05-05', '2015-06-06T12:20'])\n\n\n'''\nDesired Output\n\n0   2010-01-01 00:00:00\n1   2011-02-02 00:00:00\n2   2012-03-03 00:00:00\n3   2013-04-04 00:00:00\n4   2014-05-05 00:00:00\n5   2015-06-06 12:20:00\n'''\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using pands to_datetime\npd.to_datetime(ser)\n\n# using dateutil parse\nfrom dateutil.parser import parse\nser.map(lambda x: parse(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q22'></a>\n**22. How to get the day of month, week number, day of year and day of week from a series of date strings?**\n\nGet the day of month, week number, day of year and day of week from ser.\n\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\nser = pd.Series(['01 Jan 2010', '02-02-2011', '20120303', '2013/04/04', '2014-05-05', '2015-06-06T12:20'])\n\n'''\nDesired output\n\nDate:  [1, 2, 3, 4, 5, 6]\nWeek number:  [53, 5, 9, 14, 19, 23]\nDay num of year:  [1, 33, 63, 94, 125, 157]\nDay of week:  ['Friday', 'Wednesday', 'Saturday', 'Thursday', 'Monday', 'Saturday']\n'''\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# day\npd.to_datetime(ser).dt.day.to_list()\n# week\npd.to_datetime(ser).dt.week.to_list()\n# another method\npd.to_datetime(ser).dt.weekofyear.to_list()\n# day of year\npd.to_datetime(ser).dt.dayofyear.to_list()\n# day of week in words\nweek_dict = {0:\"Monday\", 1:\"Tuesday\", 2:\"Wednesday\", 3:\"Thursday\", 4:\"Friday\", 5:\"Saturday\", 6:\"Sunday\"}\npd.to_datetime(ser).dt.dayofweek.map(week_dict).to_list()\n# another method\npd.to_datetime(ser).dt.weekday_name.to_list()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q23'></a>\n\n**23. How to convert year-month string to dates corresponding to the 4th day of the month?**\n\nChange ser to dates that start with 4th of the respective months.\n\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\nser = pd.Series(['Jan 2010', 'Feb 2011', 'Mar 2012'])\n\n'''\nDesired Output\n\n0   2010-01-04\n1   2011-02-04\n2   2012-03-04\ndtype: datetime64[ns]\n\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# solution using parser\nfrom dateutil.parser import parse\nser.map(lambda x: parse('04 ' + x))\n\n# another solution\n\nfrom dateutil.parser import parse\n# Parse the date\nser_ts = ser.map(lambda x: parse(x))\n\n# Construct date string with date as 4\nser_datestr = ser_ts.dt.year.astype('str') + '-' + ser_ts.dt.month.astype('str') + '-' + '04'\n\n# Format it.\n[parse(i).strftime('%Y-%m-%d') for i in ser_datestr]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q24'></a>\n\n**24. How to filter words that contain atleast 2 vowels from a series?**\n\nFrom ser, extract words that contain atleast 2 vowels.\n\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\nser = pd.Series(['Apple', 'Orange', 'Plan', 'Python', 'Money'])\n\n'''\nDesired Output\n\n\n0     Apple\n1    Orange\n4     Money\ndtype: object\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using nested loops\nvowels = list(\"aeiou\")\nlist_ = []\nfor w in ser:\n    c = 0\n    for l in list(w.lower()):\n        if l in vowels:\n            c += 1\n    if c >= 2:\n        print(w)\n        list_.append(w)\n\nser[ser.isin(list_)]\n\n# another solution using counter\n\nfrom collections import Counter\nmask = ser.map(lambda x: sum([Counter(x.lower()).get(i, 0) for i in list('aeiou')]) >= 2)\nser[mask]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q25'></a>\n\n**25. How to filter valid emails from a series?**\n\nExtract the valid emails from the series emails. The regex pattern for valid emails is provided as reference.\n\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\nemails = pd.Series(['buying books at amazom.com', 'rameses@egypt.com', 'matt@t.co', 'narendra@modi.com'])\npattern ='[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Za-z]{2,4}'\n\n'''\nDesired Output\n\n1    rameses@egypt.com\n2            matt@t.co\n3    narendra@modi.com\ndtype: object\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using powerful regex\nimport re\nre_ = re.compile(pattern)\nemails[emails.str.contains(pat = re_, regex = True)]\n\n# other solutions\npattern ='[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Za-z]{2,4}'\nmask = emails.map(lambda x: bool(re.match(pattern, x)))\nemails[mask]\n\n# using str.findall\nemails.str.findall(pattern, flags=re.IGNORECASE)\n\n# using list comprehension\n[x[0] for x in [re.findall(pattern, email) for email in emails] if len(x) > 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q26'></a>\n\n**26. How to get the mean of a series grouped by another series?**\n\nCompute the mean of weights of each fruit.\n\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# doesn't incluide the upper limit\nfruit = pd.Series(np.random.choice(['apple', 'banana', 'carrot'], 10))\nfruit\nweights = pd.Series(np.linspace(1, 10, 10))\nweights\n#print(weights.tolist())\n#print(fruit.tolist())\n\n'''\nDesired output\n\n# values can change due to randomness\napple     6.0\nbanana    4.0\ncarrot    5.8\ndtype: float64\n'''\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using pandas groupby\ndf = pd.concat([fruit, weights], axis = 1)\ndf\ndf.groupby(0).mean()\n\n# use one list to calculate a kpi from another\nweights.groupby(fruit).mean()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q27'></a>\n\n**27. How to compute the euclidean distance between two series?**\n\nCompute the [euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance) between series (points) p and q, without using a packaged formula.\n\n[Go back to the table of contents](#table_of_contents)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Input\np = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\nq = pd.Series([10, 9, 8, 7, 6, 5, 4, 3, 2, 1])\n'''\nDesired Output\n\n18.165\n'''\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using list comprehension\nsuma = np.sqrt(np.sum([(p - q)**2 for p, q in zip(p, q)]))\nsuma\n\n# using series one to one operation\nsum((p - q)**2)**.5\n\n# using numpy\nnp.linalg.norm(p-q)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q28'></a>\n\n**28. How to find all the local maxima (or peaks) in a numeric series?**\n\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\nser = pd.Series([2, 10, 3, 4, 9, 10, 2, 7, 3])\n\n'''\nDesired output\n\narray([1, 5, 7])\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using pandas shift\nlocal_max = ser[(ser.shift(1) < ser) & (ser.shift(-1) < ser)]\nlocal_max.index\n\n# using numpy\ndd = np.diff(np.sign(np.diff(ser)))\ndd\npeak_locs = np.where(dd == -2)[0] + 1\npeak_locs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q29'></a>\n\n**29. How to replace missing spaces in a string with the least frequent character?**\n\nReplace the spaces in my_str with the least frequent character.\n\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\nmy_str = 'dbc deb abed ggade'\n\n'''\nDesired Output\n\n'dbccdebcabedcggade'  # least frequent is 'c'\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using Counter\nfrom collections import Counter\nmy_str_ = my_str\nCounter_ = Counter(list(my_str_.replace(\" \", \"\")))\nCounter_\nminimum = min(Counter_, key = Counter_.get)\n\nprint(my_str.replace(\" \", minimum))\n\n# using pandas\nser = pd.Series(list(my_str.replace(\" \", \"\")))\nser.value_counts()\nminimum = list(ser.value_counts().index)[-1]\nminimum\nprint(my_str.replace(\" \", minimum))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q30'></a>\n\n**30. How to create a TimeSeries starting ‘2000-01-01’ and 10 weekends (saturdays) after that having random numbers as values?**\n\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nDesired Output\nvalues can be random\n\n2000-01-01    4\n2000-01-08    1\n2000-01-15    8\n2000-01-22    4\n2000-01-29    4\n2000-02-05    2\n2000-02-12    4\n2000-02-19    9\n2000-02-26    6\n2000-03-04    6\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dti = pd.Series(pd.date_range('2000-01-01', periods=10, freq='W-SAT'))\nrandom_num = pd.Series([np.random.randint(1, 10) for i in range(10)])\n\n\ndf = pd.concat({\"Time\":dti, \"Numbers\":random_num}, axis = 1)\ndf\n\n# for more about time series functionality \n# https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#timeseries-offset-aliases\n\n# another solution just using pandas Series\nser = pd.Series(np.random.randint(1,10,10), pd.date_range('2000-01-01', periods=10, freq='W-SAT'))\nser","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q31'></a>\n\n**31. How to fill an intermittent time series so all missing dates show up with values of previous non-missing date?**\n\nser has missing dates and values. Make all missing dates appear and fill up with value from previous date.\n\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\nser = pd.Series([1,10,3,np.nan], index=pd.to_datetime(['2000-01-01', '2000-01-03', '2000-01-06', '2000-01-08']))\n\n'''\nDesired Output\n\n2000-01-01     1.0\n2000-01-02     1.0\n2000-01-03    10.0\n2000-01-04    10.0\n2000-01-05    10.0\n2000-01-06     3.0\n2000-01-07     3.0\n2000-01-08     NaN\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Solution 1\n# first let's fill the missing dates\nindx = pd.date_range(\"2000-01-01\", \"2000-01-08\")\n# now let's reindex the series ser with the new index\n# we have to reasing back to ser\nser = ser.reindex(indx)\n# lastly let's populate the missing values\nser.fillna(method = \"ffill\")\n\n# Solution 2\nser = pd.Series([1,10,3,np.nan], index=pd.to_datetime(['2000-01-01', '2000-01-03', '2000-01-06', '2000-01-08']))\nser.resample('D').ffill()  # fill with previous value\nser.resample('D').bfill()  # fill with next value\nser.resample('D').bfill().ffill()  # fill next else prev value\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q32'></a>\n\n**32. How to compute the autocorrelations of a numeric series?**\n\nCompute autocorrelations for the first 10 lags of ser. Find out which lag has the largest correlation.\n\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\nser = pd.Series(np.arange(20) + np.random.normal(1, 10, 20))\n\n'''\nDesired Output\n\n# values will change due to randomness\n[0.29999999999999999, -0.11, -0.17000000000000001, 0.46000000000000002, 0.28000000000000003, -0.040000000000000001, -0.37, 0.41999999999999998, 0.47999999999999998, 0.17999999999999999]\nLag having highest correlation:  9\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using pandas autocorr\n# ser.autocorr(lag = 10)\n\n# solution using list comprehension\nautocorrelations = [ser.autocorr(i).round(2) for i in range(11)]\nprint(autocorrelations[1:])\nprint('Lag having highest correlation: ', np.argmax(np.abs(autocorrelations[1:]))+1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q33'></a>\n\n**33. How to import only every nth row from a csv file to create a dataframe?**\n\nImport every 50th row of BostonHousing dataset as a dataframe.\n\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data comes without headers, but we searched for it\nnames = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n\n# pure Python implementation\nwith open(\"/kaggle/input/boston-house-prices/housing.csv\") as f:\n    data = f.read()\n    nth_rows = []\n    for i, rows in enumerate(data.split(\"\\n\")):\n        if i%50 == 0:\n            nth_rows.append(rows)\n            \n# nth_rows is a list of strings separated by blank spaces \" \"\n# the next list comprehension will do the trick\n\nnth_rows[0]\ndata_ = [nth_rows[i].split() for i in range(len(nth_rows))]\ndf = pd.DataFrame(data_, columns=names)\ndf\n\n# other solutions\n\n# Solution 2: Use chunks and for-loop\n# df = pd.read_csv(\"/kaggle/input/boston-house-prices/housing.csv\", chunksize=50)\n# df2 = pd.DataFrame()\n# for chunk in df:\n#     df2 = df2.append(chunk.iloc[0,:])\n# df2\n\n# Solution 3: Use chunks and list comprehension\n# df = pd.read_csv(\"/kaggle/input/boston-house-prices/housing.csv\", chunksize=50)\n# df2 = pd.concat([chunk.iloc[0] for chunk in df], axis=1)\n# df2 = df2.transpose()\n# df2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q34'></a>\n\n**34. How to change column values when importing csv to a dataframe?**\n\nImport the boston housing dataset, but while importing change the 'medv' (median house value) column so that values < 25 becomes ‘Low’ and > 25 becomes ‘High’.\n\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# first let's import using the previuos code and save as a normal csv\n\nnames = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\nwith open(\"/kaggle/input/boston-house-prices/housing.csv\") as f:\n    data = f.read()\n    nth_rows = []\n    for i, rows in enumerate(data.split(\"\\n\")):\n        nth_rows.append(rows)\n\ndata_ = [nth_rows[i].split() for i in range(len(nth_rows))]\n\ndf = pd.DataFrame(data_, columns=names)\ndf.head()\ndf.to_csv(\"housing_preprocessed.csv\")\ndel df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now let's start importing as normal and use converters to convert the values\n# skipfooter because we had the last rows with nan values and index_col to specify that the first column is the index\ndf = pd.read_csv(\"housing_preprocessed.csv\",  index_col = 0, skipfooter=1,  converters = {\"MEDV\": lambda x: \"HIGH\" if float(x) >= 25 else \"LOW\"})\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q35'></a>\n\n**35. How to create a dataframe with rows as strides from a given series?**\n\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\nL = pd.Series(range(15))\n\n'''\nDesired Output\n\narray([[ 0,  1,  2,  3],\n       [ 2,  3,  4,  5],\n       [ 4,  5,  6,  7],\n       [ 6,  7,  8,  9],\n       [ 8,  9, 10, 11],\n       [10, 11, 12, 13]])\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using slicing\n# let's generate a list of indexes we need to use\n# outputs array([ 0,  2,  4,  6,  8, 10, 12, 14])\nindex_ = np.arange(0, 15, 2)\nindex_\nmy_list = []\nfor i in range(6):\n    my_list.append(list(L[index_[i]:index_[i+2]]))\nnp.array(my_list)\n\n# above code as list comprehension\nnp.array([L[index_[i]:index_[i+2]] for i in range(6)])\n\n# another solution\ndef gen_strides(a, stride_len=5, window_len=5):\n    n_strides = ((a.size-window_len)//stride_len) + 1\n    return np.array([a[s:(s+window_len)] for s in np.arange(0, a.size, stride_len)[:n_strides]])\n\ngen_strides(L, stride_len=2, window_len=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q36'></a>\n\n**36. How to import only specified columns from a csv file?**\n\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\n        \n# code that generates the housing_preprocessed.csv file\nnames = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\nwith open(\"/kaggle/input/boston-house-prices/housing.csv\") as f:\n    data = f.read()\n    nth_rows = []\n    for i, rows in enumerate(data.split(\"\\n\")):\n        nth_rows.append(rows)\n\ndata_ = [nth_rows[i].split() for i in range(len(nth_rows))]\n\ndf = pd.DataFrame(data_, columns=names)\ndf.to_csv(\"housing_preprocessed.csv\")\ndel df\n\n# use the /kaggle/input/boston-house-prices/housing_preprocessed.csv file\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file = \"housing_preprocessed.csv\"\n# using index\ndf = pd.read_csv(file, usecols = [1, 2, 4], skipfooter=1)\ndf.head()\n# using column names\ndf = pd.read_csv(file, usecols = [\"CRIM\", \"ZN\", \"CHAS\"])\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q37'></a>\n\n**37. How to get the nrows, ncolumns, datatype, summary stats of each column of a dataframe? Also get the array and list equivalent.**\n\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\n# use the \"housing_preprocessed.csv\" file\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"housing_preprocessed.csv\", index_col=0 ,skipfooter=1)\n# number of rows and columns\ndf.shape\n\n# each type of column\ndf.dtypes\n\n# a more general view of the earlier code\ndf.info()\n\n# how many columns under each dtype\ndf.get_dtype_counts()\ndf.dtypes.value_counts()\n\n# all the statistics\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q38'></a>\n\n**38. How to extract the row and column number of a particular cell with given criterion?**\n\n[Go back to the table of contents](#table_of_contents)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\n# use the \"housing_preprocessed.csv\" file","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# solution 1\ndf = pd.read_csv(\"housing_preprocessed.csv\", skipfooter=1, index_col=0)\n# let's get the maximum value\nmax_tax = df[\"TAX\"].max()\nmax_tax\n\n# now let's find the column and cell that has the maximum value\ndf[df[\"TAX\"] == max_tax]\n\n# solution 2\ndf.loc[df[\"TAX\"] == np.max(df[\"TAX\"]), [\"CRIM\", \"ZN\", \"TAX\"]]\n\n# solution 3\n# get the row and column number\nrow, col = np.where(df.values == np.max(df[\"TAX\"]))\nfor i, j in zip(row, col):\n    print(i , j)\n    \n# Get the value\ndf.iat[row[0], col[0]]\ndf.iloc[row[0], col[0]]\n\n# Alternates\ndf.at[row[0], 'TAX']\ndf.get_value(row[0], 'TAX')\n\n# The difference between `iat` - `iloc` vs `at` - `loc` is:\n# `iat` snd `iloc` accepts row and column numbers. \n# Whereas `at` and `loc` accepts index and column names.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q39'></a>\n\n**39. How to rename a specific columns in a dataframe?**\n\n[Go back to the table of contents](#table_of_contents)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\n# Rename the column Type as CarType in df and replace the ‘.’ in column names with ‘_’.\ncars93 = pd.read_csv(\"../input/cars93/Cars93.csv\", index_col=0)\ncars93.head()\n\n'''\nDesired Output\n\nIndex(['Manufacturer', 'Model', 'CarType', 'Min_Price', 'Price', 'Max_Price',\n        'MPG_city', 'MPG_highway', 'AirBags', 'DriveTrain', 'Cylinders',\n        'EngineSize', 'Horsepower', 'RPM', 'Rev_per_mile', 'Man_trans_avail',\n        'Fuel_tank_capacity', 'Passengers', 'Length', 'Wheelbase', 'Width',\n        'Turn_circle', 'Rear_seat_room', 'Luggage_room', 'Weight', 'Origin',\n        'Make'],\n       dtype='object')\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Solution 1: in 2 steps\n# Step1\n# first let's rename the Type to CarType\ncars93 = pd.read_csv(\"../input/cars93/Cars93.csv\", index_col=0)\ncars93.rename(columns={\"Type\":\"CarType\"}, inplace = True)\ncols = cars93.columns\n# or\ndf.columns.values[2] = \"CarType\"\n# Step2\n# replace the \".\" with \"-\"\ncols = list(map(lambda x: x.replace(\".\", \"_\"), cols))\ncars93.columns = cols\ncars93.head()\n\n# Solution 2: working only with lists\ncars93 = pd.read_csv(\"../input/cars93/Cars93.csv\", index_col=0)\ncols = cars93.columns\ncols = list(map(lambda x: x.replace(\".\", \"_\"), cols))\ncols[cols.index(\"Type\")] = \"CarType\"\ncars93.columns = cols\ncars93.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q40'></a>\n\n**40. How to check if a dataframe has any missing values?**\n\n[Go back to the table of contents](#table_of_contents)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\ndf = pd.read_csv(\"../input/cars93/Cars93.csv\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Solution 1\nprint(\"Our df has a total of {} null values\".format(df.isnull().sum().sum()))\nprint()\n\n# Solution 2\ndf.isnull().values.any()\nprint()\n\n# Solution 3\n# A more detailed one\ndef report_nulls(df):\n    '''\n    Show a fast report of the DF.\n    '''\n    rows = df.shape[0]\n    columns = df.shape[1]\n    null_cols = 0\n    list_of_nulls_cols = []\n    for col in list(df.columns):\n        null_values_rows = df[col].isnull().sum()\n        null_rows_pcn = round(((null_values_rows)/rows)*100, 2)\n        col_type = df[col].dtype\n        if null_values_rows > 0:\n            print(\"The column {} has {} null values. It is {}% of total rows.\".format(col, null_values_rows, null_rows_pcn))\n            print(\"The column {} is of type {}.\\n\".format(col, col_type))\n            null_cols += 1\n            list_of_nulls_cols.append(col)\n    null_cols_pcn = round((null_cols/columns)*100, 2)\n    print(\"The DataFrame has {} columns with null values. It is {}% of total columns.\".format(null_cols, null_cols_pcn))\n    return list_of_nulls_cols\n\nreport_nulls(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q41'></a>\n\n**41. How to count the number of missing values in each column?**\n\nCount the number of missing values in each column of df. Which column has the maximum number of missing values?\n\n[Go back to the table of contents](#table_of_contents)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\ndf = pd.read_csv(\"../input/cars93/Cars93.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Solution 1\ndf_null = pd.DataFrame(df.isnull().sum())\ndf_null[df_null[0] > 0][0].argmax()\ndf_null[df_null[0] > 0][0].idxmax()\n\n# Solution 2\n# find the total number of nulls per column\nn_missings_each_col = df.apply(lambda x: x.isnull().sum())\n\n# find the maximum nulls\nn_missings_each_col.argmax()\nn_missings_each_col.idxmax()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q42'></a>\n\n**42. How to replace missing values of multiple numeric columns with the mean?**\n\nReplace missing values in Luggage.room columns with their respective mean.\n\n[Go back to the table of contents](#table_of_contents)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\ndf = pd.read_csv(\"../input/cars93/Cars93.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Solution 1\nbeg_null = df.isnull().sum().sum()\nprint(beg_null)\n# notice that we have filtering the columns  as a list.\ndf[[\"Luggage.room\"]] = df[[\"Luggage.room\"]].apply(lambda x: x.fillna(x.mean()))\nend_null = df.isnull().sum().sum()\nprint(end_null)\n\nprint(\"We have got rid of {} null values, filling them with the mean.\".format(beg_null - end_null))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q43'></a>\n\n**43. How to use apply function on existing columns with global variables as additional arguments?**\n\nIn df, use apply method to replace the missing values in Rear.seat.room with mean Luggage.room with median by passing an argument to the function.\n\n[Go back to the table of contents](#table_of_contents)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\ndf = pd.read_csv(\"../input/cars93/Cars93.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\ndf = pd.read_csv(\"../input/cars93/Cars93.csv\")\n\n# Solution 1\nprint(\"We have a total of {} nulls\".format(df.isnull().sum().sum()))\n\nd = {'Rear.seat.room': np.nanmean, 'Luggage.room': np.nanmedian}\ndf[['Rear.seat.room', 'Luggage.room']] = df[['Rear.seat.room', 'Luggage.room']].apply(lambda x, d: x.fillna(d[x.name](x)), args=(d, ))\n\nprint(\"We have a total of {} nulls\".format(df.isnull().sum().sum()))\n\ndf[\"Rear.seat.room\"].sum()\ndf[\"Luggage.room\"].sum()\n\n\n# Solution 2\n# impor the df\ndf = pd.read_csv(\"../input/cars93/Cars93.csv\")\n\n# check nulls\nprint(\"We have a total of {} nulls\".format(df.isnull().sum().sum()))\n\n# define a custom function\ndef num_inputer(x, strategy):\n    if strategy.lower() == \"mean\":\n        x = x.fillna(value = np.nanmean(x))\n    if strategy.lower() == \"median\":\n        x = x.fillna(value = np.nanmedian(x))\n    return x\n\n# apply the custon function and using args whe can pass the strategy we want\ndf['Rear.seat.room'] = df[['Rear.seat.room']].apply(num_inputer, args = [\"mean\"])\ndf['Luggage.room'] = df[['Luggage.room']].apply(num_inputer, args = [\"median\"])\n\n# check for nulls\nprint(\"We have a total of {} nulls\".format(df.isnull().sum().sum()))\n\ndf[\"Rear.seat.room\"].sum()\ndf[\"Luggage.room\"].sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q44'></a>\n\n**44. How to select a specific column from a dataframe as a dataframe instead of a series?**\n\nGet the first column (a) in df as a dataframe (rather than as a Series).\n\n[Go back to the table of contents](#table_of_contents)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":" # input\ndf = pd.DataFrame(np.arange(20).reshape(-1, 5), columns=list('abcde'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Solution\n# using to_frame()\ntype(df[\"a\"].to_frame())\n# using pandas DataFrame\ntype(pd.DataFrame(df[\"a\"]))\n\n# Other solutions\n# Solution\ntype(df[['a']])\ntype(df.loc[:, ['a']])\ntype(df.iloc[:, [0]])\n\n# This returns a series\n# Alternately the following returns a Series\ntype(df.a)\ntype(df['a'])\ntype(df.loc[:, 'a'])\ntype(df.iloc[:, 1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q45'></a>\n\n**45. How to change the order of columns of a dataframe?**\n\nActually 3 questions.\n\n1. In df, interchange columns 'a' and 'c'.\n\n2. Create a generic function to interchange two columns, without hardcoding column names.\n\n3. Sort the columns in reverse alphabetical order, that is colume 'e' first through column 'a' last.\n\n[Go back to the table of contents](#table_of_contents)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\ndf = pd.DataFrame(np.arange(20).reshape(-1, 5), columns=list('abcde'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Solution to question 1\n# we pass a list with the custom names BUT THIS DOESN'T change in place\ndf = pd.DataFrame(np.arange(20).reshape(-1, 5), columns=list('abcde'))\ndf[[\"c\", \"b\", \"a\", \"d\", \"e\"]]\ndf\n\n# if we reasing that this will work\ndf = df[[\"c\", \"b\", \"a\", \"d\", \"e\"]]\ndf\n\n# Solution to question 2\ndef change_cols(df, col1, col2):\n    df_columns = df.columns.to_list()\n    index1 = df_columns.index(col1)\n    index2 = df_columns.index(col2)\n    # swaping values\n    df_columns[index1], df_columns[index2] = col1, col2\n    \n    return df[df_columns]\n\n\ndf = change_cols(df, \"b\", \"e\")\ndf\n    \n\n# Solution to question 3\ndf = pd.DataFrame(np.arange(20).reshape(-1, 5), columns=list('abcde'))\ncol_list = list(df.columns)\ncol_list_reversed = col_list[::-1]\ncol_list\ncol_list_reversed\n# using the trick from solution 1\ndf = df[col_list_reversed]\ndf\n\n\nprint(\"Solution from the website\")\nprint(\"-------------------------\")\n# Others solution from the website\n\n# Input\ndf = pd.DataFrame(np.arange(20).reshape(-1, 5), columns=list('abcde'))\n\n# Solution Q1\ndf[list('cbade')]\n\n# Solution Q2 - No hard coding\ndef switch_columns(df, col1=None, col2=None):\n    colnames = df.columns.tolist()\n    i1, i2 = colnames.index(col1), colnames.index(col2)\n    colnames[i2], colnames[i1] = colnames[i1], colnames[i2]\n    return df[colnames]\n\ndf1 = switch_columns(df, 'a', 'c')\n\n# Solution Q3\ndf[sorted(df.columns)]\n# or\ndf.sort_index(axis=1, ascending=False, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q46'></a>\n\n**46. How to set the number of rows and columns displayed in the output?**\n\nChange the pandas display settings on printing the dataframe df it shows a maximum of 10 rows and 10 columns.\n\n\n[Go back to the table of contents](#table_of_contents)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\ndf = pd.read_csv(\"../input/cars93/Cars93.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we use set_option to set the maximun rows and columns to display\npd.set_option(\"display.max_columns\",10)\npd.set_option(\"display.max_rows\",10)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q47'></a>\n\n**47. How to format or suppress scientific notations in a pandas dataframe?**\n\nSuppress scientific notations like ‘e-03’ in df and print upto 4 numbers after decimal.\n\n\n[Go back to the table of contents](#table_of_contents)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\ndf = pd.DataFrame(np.random.random(5)**10, columns=['random'])\n\n'''\nDesired Output\n#>    random\n#> 0  0.0035\n#> 1  0.0000\n#> 2  0.0747\n#> 3  0.0000\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Initial DF\")\ndf\nprint(\"Using solution 1\")\n# Solution 1\ndf.round(4)\ndf\npd.reset_option('^display.', silent=True)\n\nprint(\"Using solution 2\")\n# Solution 2\ndf.apply(lambda x: '%.4f' %x, axis=1).to_frame()\ndf\npd.reset_option('^display.', silent=True)\n\nprint(\"Using solution 3\")\n# Solution 3\npd.set_option('display.float_format', lambda x: '%.4f'%x)\ndf\npd.reset_option('^display.', silent=True)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q48'></a>\n\n**48. How to format all the values in a dataframe as percentages?**\n\nFormat the values in column 'random' of df as percentages.\n\n\n[Go back to the table of contents](#table_of_contents)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\ndf = pd.DataFrame(np.random.random(4), columns=['random'])\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Solution 1\n# Using style.format we can pass a dictionary to each column and display as we want\nout = df.style.format({\n    'random': '{0:.2%}'.format,\n})\nout\n\n# This applies to all the df\npd.options.display.float_format = '{:,.2f}%'.format\n# to get the % multiply by 100\ndf*100\npd.reset_option('^display.', silent=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q49'></a>\n\n**49. How to filter every nth row in a dataframe?**\n\nFrom df, filter the 'Manufacturer', 'Model' and 'Type' for every 20th row starting from 1st (row 0).\n\n[Go back to the table of contents](#table_of_contents)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\ndf = pd.read_csv(\"../input/cars93/Cars93.csv\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First let's import only the columns we need\ndf = pd.read_csv(\"../input/cars93/Cars93.csv\", usecols=[\"Manufacturer\", \"Model\", \"Type\"])\n\n# Solution 1\n# Using normal python slicing\ndf[::20]\n\ndf = pd.read_csv(\"../input/cars93/Cars93.csv\", usecols=[\"Manufacturer\", \"Model\", \"Type\"])\n\n# Solution 2\n# Using iloc\ndf.iloc[::20, :][['Manufacturer', 'Model', 'Type']]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q50'></a>\n\n**50. How to create a primary key index by combining relevant columns?**\n\nIn df, Replace NaNs with ‘missing’ in columns 'Manufacturer', 'Model' and 'Type' and create a index as a combination of these three columns and check if the index is a primary key.\n\n[Go back to the table of contents](#table_of_contents)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\ndf = pd.read_csv(\"../input/cars93/Cars93.csv\")\ndf\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Solution\ndf = pd.read_csv(\"../input/cars93/Cars93.csv\", usecols=[\"Manufacturer\", \"Model\", \"Type\", \"Min.Price\", \"Max.Price\"])\n\n# let's check if we have null\ndf.isnull().sum().sum()\ndf.fillna(\"missing\")\n# create new index\ndf[\"new_index\"] = df[\"Manufacturer\"] + df[\"Model\"] + df[\"Type\"]\n# set new index\ndf.set_index(\"new_index\", inplace = True)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q51'></a>\n\n**51. How to get the row number of the nth largest value in a column?**\n\nFind the row position of the 5th largest value of column 'a' in df.\n\n[Go back to the table of contents](#table_of_contents)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\ndf = pd.DataFrame(np.random.randint(1, 30, 30).reshape(10,-1), columns=list('abc'))\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Solution 1\n\n# argsort give the index of the smallest to largest number in an array\n# arg_sort[0] is the index of the smallest number in df[\"a\"]\narg_sort = df[\"a\"].argsort()\n\n#arg_sort.to_frame()\n#arg_sort[0]\n\n# now let's sort by arg_sort\n#df\ndf = df.iloc[arg_sort]\ndf[\"arg_sort\"] = arg_sort\ndf\nn_largest = 5\nprint(\"The {} largest values in our DF is at row/index {} and the value is {}\".format(n_largest, (df[df[\"arg_sort\"] == (n_largest-1)].index[0]), df[df[\"arg_sort\"] == (n_largest-1)][\"a\"].iloc[0]))\n\n# Shorter solution\nn = 5\n# select column, argsort, inders (largest to smallest) and select the n largest\ndf['a'].argsort()[::-1][n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q52'></a>\n\n**52. How to find the position of the nth largest value greater than a given value?**\n\nIn ser, find the position of the 2nd largest value greater than the mean.\n\n[Go back to the table of contents](#table_of_contents)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\nser = pd.Series(np.random.randint(1, 100, 15))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Solution using argsort and boolean filtering of pandas series\n# I understood that I wanted the second largest of all values that is greter than the mean\n# so I sorted\n#ser\nsorted_ser = ser[ser.argsort()[::-1]]\n#sorted_ser\nsorted_ser[sorted_ser > sorted_ser.mean()].index[1]\n\n# If you understood that the 2 value you encounter that is bigger than the mean\n# This is the correct solution\nprint('ser: ', ser.tolist(), 'mean: ', round(ser.mean()))\nnp.argwhere(ser > ser.mean())[1]\n\n# Another solution\nser[ser > ser.mean()].index[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<a id = 'q53'></a>\n\n**53. How to get the last n rows of a dataframe with row sum > 100?**\n\nGet the last two rows of df whose row sum is greater than 100.\n\n[Go back to the table of contents](#table_of_contents)\n\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\ndf = pd.DataFrame(np.random.randint(10, 40, 60).reshape(-1, 4))\ndf1 = df.copy(deep = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Solution 1\ndf[\"sum\"] = df.sum(axis = 1)\ndf\n\nprint(\"The index of the rows that are greater than 100 are {}\".format((df[df[\"sum\"] > 100].index).to_list()[-2:]))\n\n# Solution 2 using numpy\nrowsums = df1.apply(np.sum, axis=1)\n\n# last two rows with row sum greater than 100\nlast_two_rows = df1.iloc[np.where(rowsums > 100)[0][-2:], :]\nlast_two_rows","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<a id = 'q54'></a>\n\n**54. How to find and cap outliers from a series or dataframe column?**\n\nReplace all values of ser in the lower 5%ile and greater than 95%ile with respective 5th and 95th %ile value.\n\n[Go back to the table of contents](#table_of_contents)\n\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\nser = pd.Series(np.logspace(-2, 2, 30))\nser1 = ser.copy(deep = True)\nser2 = ser.copy(deep = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Solution 1\n# get the quantiles values\nquantiles = np.quantile(ser, [0.05, 0.95])\nser\n\n# filter ser using numpy to know where the values are below or greater than 5% or 95% and replace the values\nser.iloc[np.where(ser < quantiles[0])] = quantiles[0]\nser.iloc[np.where(ser > quantiles[1])] = quantiles[1]\n    \n# or we can just do\nser1[ser1 < quantiles[0]] = quantiles[0]\nser1[ser1 > quantiles[1]] = quantiles[1]\n\nser1\n\n# Solution from the webpage\ndef cap_outliers(ser, low_perc, high_perc):\n    low, high = ser.quantile([low_perc, high_perc])\n    print(low_perc, '%ile: ', low, '|', high_perc, '%ile: ', high)\n    ser[ser < low] = low\n    ser[ser > high] = high\n    return(ser)\n\ncapped_ser = cap_outliers(ser2, .05, .95)\nser2\ncapped_ser","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<a id = 'q55'></a>\n\n**55. How to reshape a dataframe to the largest possible square after removing the negative values?**\n\nReshape df to the largest possible square with negative values removed. Drop the smallest values if need be. \nThe order of the positive numbers in the result should remain the same as the original.\n\n[Go back to the table of contents](#table_of_contents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\ndf = pd.DataFrame(np.random.randint(-20, 50, 100).reshape(10,-1))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This solution sorts the values.\n# Not want we want\n# my_array = np.array(df.values.reshape(-1, 1))\n# my_array = my_array[my_array > 0]\n# my_array.shape[0]\n# lar_square = int(np.floor(my_array.shape[0]**0.5))\n# arg_sort = np.argsort(my_array)[::-1]\n# my_array[arg_sort][0:lar_square**2].reshape(lar_square, lar_square)\n\n\n# Correct solution\nmy_array = np.array(df.values.reshape(-1, 1)) # convert to numpy\nmy_array = my_array[my_array > 0] # filter only positive values\nlar_square = int(np.floor(my_array.shape[0]**0.5)) # find the largest square\narg_sort = np.argsort(my_array)[::-1][0:lar_square**2] # eliminate the smallest values that will prevent from converting to a square\nmy_array = np.take(my_array, sorted(arg_sort)).reshape(lar_square, lar_square) # filter the array and reshape back\nmy_array\n\n\n# Solution from the webpage\n# Step 1: remove negative values from arr\narr = df[df > 0].values.flatten()\narr_qualified = arr[~np.isnan(arr)]\n\n# Step 2: find side-length of largest possible square\nn = int(np.floor(arr_qualified.shape[0]**.5))\n\n# Step 3: Take top n^2 items without changing positions\ntop_indexes = np.argsort(arr_qualified)[::-1]\noutput = np.take(arr_qualified, sorted(top_indexes[:n**2])).reshape(n, -1)\nprint(output)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<a id = 'q56'></a>\n\n**56. How to swap two rows of a dataframe?**\n\nSwap rows 1 and 2 in df.\n\n[Go back to the table of contents](#table_of_contents)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\ndf = pd.DataFrame(np.arange(25).reshape(5, -1))\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# THIS SWAPS the columns\nprint(\"Original DataFrame\")\ndf\ntemp_col = df[1].copy(deep = True)\ndf[1], df[2] = df[2], temp_col\nprint(\"Swapped Columns DataFrame\")\ndf\n\n# # THIS SWAPS the rows\nprint(\"Original DataFrame\")\ndf\ntemp_row = df.iloc[1].copy(deep = True)\ndf.iloc[1], df.iloc[2] = df.iloc[2], temp_row\nprint(\"Swapped Rows DataFrame\")\ndf\n\n# Solution from the webpage\ndef swap_rows(df, i1, i2):\n    a, b = df.iloc[i1, :].copy(), df.iloc[i2, :].copy()\n    df.iloc[i1, :], df.iloc[i2, :] = b, a\n    return df\n\nprint(swap_rows(df, 1, 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<a id = 'q57'></a>\n\n**57. How to reverse the rows of a dataframe?**\n\nReverse all the rows of dataframe df.\n\n[Go back to the table of contents](#table_of_contents)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\ndf = pd.DataFrame(np.arange(25).reshape(5, -1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Solution 1\ndf\ndf.iloc[df.index.to_list()[::-1]]\n\n# Solutions from the webpage\n# Solution 2\ndf.iloc[::-1, :]\n\n# Solution 3\nprint(df.loc[df.index[::-1], :])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<a id = 'q58'></a>\n\n**58. How to create one-hot encodings of a categorical variable (dummy variables)?**\n\nGet one-hot encodings for column 'a' in the dataframe df and append it as columns.\n\n[Go back to the table of contents](#table_of_contents)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\ndf = pd.DataFrame(np.arange(25).reshape(5,-1), columns=list('abcde'))\n\n'''\nDesired Output\n\n   0  5  10  15  20   b   c   d   e\n0  1  0   0   0   0   1   2   3   4\n1  0  1   0   0   0   6   7   8   9\n2  0  0   1   0   0  11  12  13  14\n3  0  0   0   1   0  16  17  18  19\n4  0  0   0   0   1  21  22  23  24\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using pd.get_dummies\ndummies = pd.get_dummies(df[\"a\"])\ndf = pd.concat([dummies, df], axis = 1)\ndf\n\n# Solution from the webpage\n# in one line\ndf_onehot = pd.concat([pd.get_dummies(df['a']), df[list('bcde')]], axis=1)\ndf_onehot","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<a id = 'q59'></a>\n\n**59. Which column contains the highest number of row-wise maximum values?**\n\nObtain the column name with the highest number of row-wise maximum’s in df.\n\n[Go back to the table of contents](#table_of_contents)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\ndf = pd.DataFrame(np.random.randint(1,100, 40).reshape(10, -1))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Solution 1\ndef get_col(df):\n    columns = list(df.columns)\n    df[\"col_index_with_max\"] = \"\"\n    for i in range(len(df)):\n        row_values = list(df.iloc[i, :-1].values)\n        max_value = np.max(row_values)\n        col_index = row_values.index(max_value)\n        df[\"col_index_with_max\"].iloc[i] = col_index\n\nget_col(df)\n\ndf\nprint(\"The col with maximum amont of maximun per row if {} with a total of {} maximus\".format(df.groupby(\"col_index_with_max\").size()[::-1].index[0], \\\n                                                                                              df.groupby(\"col_index_with_max\").size()[::-1].values[0]))\n\n# Solution 2\n# Another much more elegant solution from the webpage\nprint('Column with highest row maxes: ', df.apply(np.argmax, axis=1).value_counts().index[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<a id = 'q60'></a>\n\n**60. How to create a new column that contains the row number of nearest column by euclidean distance?**\n\nCreate a new column such that, each row contains the row number of nearest row-record by euclidean distance.\n\n[Go back to the table of contents](#table_of_contents)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\ndf = pd.DataFrame(np.random.randint(1,100, 40).reshape(10, -1), columns=list('pqrs'), index=list('abcdefghij'))\n\n'''\nDesired Output\n\ndf\n#    p   q   r   s nearest_row   dist\n# a  57  77  13  62           i  116.0\n# b  68   5  92  24           a  114.0\n# c  74  40  18  37           i   91.0\n# d  80  17  39  60           i   89.0\n# e  93  48  85  33           i   92.0\n# f  69  55   8  11           g  100.0\n# g  39  23  88  53           f  100.0\n# h  63  28  25  61           i   88.0\n# i  18   4  73   7           a  116.0\n# j  79  12  45  34           a   81.0\n\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#######################################################################################################################################\n# Solution 1\n# input\ndf = pd.DataFrame(np.random.randint(1,100, 40).reshape(10, -1), columns=list('pqrs'), index=list('abcdefghij'))\n\n# place holders\ncorr_list = []\nindex_list = []\n\n# temporary var\nmax_corr = 0\ncurrent_index = \"\"\n\n# nested loop to calculate\nfor i in range(len(df)):\n    for j in range(len(df)):\n        if i == j:\n            pass\n        else:\n            # distance\n            curr_corr = sum((df.iloc[i] - df.iloc[j])**2)**.5\n            # correlation\n            #curr_corr = df.iloc[i].corr(df.iloc[j])\n            if curr_corr >= max_corr:\n                max_corr = curr_corr\n                current_index = list(df.index)[j]\n                \n    corr_list.append(max_corr)\n    index_list.append(current_index)\n    \n    max_corr = 0\n    current_index = \"\"\n    \ndf[\"nearest_row\"] = index_list\ndf[\"dist\"] = corr_list\ndf\ndf.drop([\"nearest_row\", \"dist\"], axis = 1, inplace = True)\n\n#######################################################################################################################################\n\n# Solution from the webpage\n# init outputs\nnearest_rows = []\nnearest_distance = []\n\n# iterate rows.\nfor i, row in df.iterrows():\n    curr = row\n    rest = df.drop(i)\n    e_dists = {}  # init dict to store euclidean dists for current row.\n    # iterate rest of rows for current row\n    for j, contestant in rest.iterrows():\n        # compute euclidean dist and update e_dists\n        e_dists.update({j: round(np.linalg.norm(curr.values - contestant.values))})\n    # update nearest row to current row and the distance value\n    nearest_rows.append(max(e_dists, key=e_dists.get))\n    nearest_distance.append(max(e_dists.values()))\n\ndf['nearest_row'] = nearest_rows\ndf['dist'] = nearest_distance\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<a id = 'q61'></a>\n\n**61. How to know the maximum possible correlation value of each column against other columns?**\n\nFor each column get the maximum possible correlation with other columns (only 1 value)\n\n[Go back to the table of contents](#table_of_contents)\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\ndf = pd.DataFrame(np.random.randint(1,100, 80).reshape(8, -1), columns=list('pqrstuvwxy'), index=list('abcdefgh'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# calculate the correlation, returns a matrix \ndf_corr = np.abs(df.corr())\n# sorted -2 because it goes from min to max\n# max = 1 because it's correlation againts each other\n# so we pick -2\nmax_corr = df_corr.apply(lambda x: sorted(x)[-2], axis = 0)\nmax_corr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<a id = 'q62'></a>\n\n**62. How to create a column containing the minimum by maximum of each row?**\n\nCompute the minimum-by-maximum for every row of df.\n\n[Go back to the table of contents](#table_of_contents)\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\ndf = pd.DataFrame(np.random.randint(1,100, 80).reshape(8, -1))\ndf1 = df.copy(deep = True)\ndf2 = df.copy(deep = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Solution 1\ndf[\"min_by_max\"] = (df.apply(min, axis = 1)/df.apply(max, axis = 1))\ndf\n\n# Other solution from the webpage\n# Solution 2\nmin_by_max = df1.apply(lambda x: np.min(x)/np.max(x), axis=1)\nmin_by_max\n# Solution 3\nmin_by_max = np.min(df2, axis=1)/np.max(df2, axis=1)\nmin_by_max","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<a id = 'q63'></a>\n\n**63. How to create a column that contains the penultimate value in each row?**\n\nCreate a new column 'penultimate' which has the second largest value of each row of df.\n\n[Go back to the table of contents](#table_of_contents)\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\ndf = pd.DataFrame(np.random.randint(1,100, 80).reshape(8, -1))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using lambda and numpy partition\ndf[\"penultimate\"] = df.apply(lambda x: np.partition(x, -2)[-2], axis = 1)\ndf\ndf.drop(\"penultimate\", inplace = True, axis = 1)\n\n# Using lambda and python lists\ndf[\"penultimate\"] = df.apply(lambda x: sorted(list(x))[-2], axis = 1)\ndf\ndf.drop(\"penultimate\", inplace = True, axis = 1)\n\n# Solution from the webpage\nout = df.apply(lambda x: x.sort_values().unique()[-2], axis=1)\ndf['penultimate'] = out\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<a id = 'q64'></a>\n\n**64. How to normalize all columns in a dataframe?**\n\n1. Normalize all columns of df by subtracting the column mean and divide by standard deviation.\n\n2. Range all columns of df such that the minimum value in each column is 0 and max is 1.\n\n**Don’t use external packages like sklearn**\n\n[Go back to the table of contents](#table_of_contents)\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\ndf = pd.DataFrame(np.random.randint(1,100, 80).reshape(8, -1))\ndf1 = df.copy(deep = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First normalization: mean and std    \ndf = df.apply(lambda x: ((x-np.mean(x))/np.std(x)), axis = 0)\ndf\n\n# min max\ndf1 = df1.apply(lambda x: ((x.max() - x)/(x.max() - x.min())).round(2))\ndf1\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<a id = 'q65'></a>\n\n**65. How to compute the correlation of each row with the suceeding row?**\n\nCompute the correlation of each row of df with its succeeding row.\n\n[Go back to the table of contents](#table_of_contents)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\ndf = pd.DataFrame(np.random.randint(1,100, 80).reshape(8, -1))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"corr\"] = 0\nfor i in range(len(df)-1):\n    \n    values1 = df.iloc[i, :-1].astype('float64')\n    values2 = df.iloc[i+1, :-1].astype('float64')\n    corr = values1.corr(values2)\n    df[\"corr\"].iloc[i] = corr\ndf\ndf.drop(\"corr\", inplace = True, axis = 1)\n\n# Solution from the webpage\n# using list comprehension\n[df.iloc[i].corr(df.iloc[i+1]).round(2) for i in range(df.shape[0])[:-1]]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<a id = 'q66'></a>\n\n**66. How to replace both the diagonals of dataframe with 0?**\n\nReplace both values in both diagonals of df with 0.\n\n[Go back to the table of contents](#table_of_contents)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\ndf = pd.DataFrame(np.random.randint(1,100, 100).reshape(10, -1))\ndf1 = df.copy(deep = True)\n\n'''\nDesired Output (might change because of randomness)\n\n#     0   1   2   3   4   5   6   7   8   9\n# 0   0  46  26  44  11  62  18  70  68   0\n# 1  87   0  52  50  81  43  83  39   0  59\n# 2  47  76   0  77  73   2   2   0  14  26\n# 3  64  18  74   0  16  37   0   8  66  39\n# 4  10  18  39  98   0   0  32   6   3  29\n# 5  29  91  27  86   0   0  28  31  97  10\n# 6  37  71  70   0   4  72   0  89  12  97\n# 7  65  22   0  75  17  10  43   0  12  77\n# 8  47   0  96  55  17  83  61  85   0  86\n# 9   0  80  28  45  77  12  67  80   7   0\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\ndf = pd.DataFrame(np.random.randint(1,100, 100).reshape(10, -1))\ndf1 = df.copy(deep = True)\n\n# Using nested loops\nprint(\"Original DF\")\ndf\nfor i in range(len(df)):\n    for j in range(len(df)):\n        if i == j:\n            df.iloc[i ,j] = 0\n            # Inverse the matrix so that we can replace the other diagonal\n            df[::-1].iloc[i, j] = 0\n\nprint(\"DF from the solution 1\")\ndf\n\n# Solution from the webpage\n# Solution\nfor i in range(df1.shape[0]):\n    df1.iat[i, i] = 0\n    df1.iat[df1.shape[0]-i-1, i] = 0\n    \nprint(\"DF from the solution 2\")\ndf1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q67'></a>\n\n**67. How to get the particular group of a groupby dataframe by key?**\n\nThis is a question related to understanding of grouped dataframe. From df_grouped, get the group belonging to 'apple' as a dataframe.\n\n[Go back to the table of contents](#table_of_contents)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\ndf = pd.DataFrame({'col1': ['apple', 'banana', 'orange'] * 3,\n                   'col2': np.random.rand(9),\n                   'col3': np.random.randint(0, 15, 9)})\n\ndf_grouped = df.groupby(['col1'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Solution 1\npd.DataFrame(df_grouped)\ndf_grouped.groups[\"apple\"]\ndf_grouped.get_group(\"apple\")\n\n# Solution 2\nfor i, dff in df_grouped:\n    if i == 'apple':\n        print(dff)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q68'></a>\n\n**68. How to get the n’th largest value of a column when grouped by another column?**\n\nIn df, find the second largest value of 'rating' for 'banana'\n\n[Go back to the table of contents](#table_of_contents)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\ndf = pd.DataFrame({'fruit': ['apple', 'banana', 'orange'] * 3,\n                   'rating': np.random.rand(9),\n                   'price': np.random.randint(0, 15, 9)})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Solution 1\ngrouped_by = df[\"rating\"].groupby(df[\"fruit\"])\ngrouped_by.get_group(\"banana\")\nlist(grouped_by.get_group(\"banana\"))[1]\n\n# Solution from the webpage\ndf_grpd = df['rating'].groupby(df.fruit)\ndf_grpd.get_group('banana')\ndf_grpd.get_group('banana').sort_values().iloc[-2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q69'></a>\n\n**69. How to compute grouped mean on pandas dataframe and keep the grouped column as another column (not index)?**\n\nIn df, Compute the mean price of every fruit, while keeping the fruit as another column instead of an index.\n\n[Go back to the table of contents](#table_of_contents)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\ndf = pd.DataFrame({'fruit': ['apple', 'banana', 'orange'] * 3,\n                   'rating': np.random.rand(9),\n                   'price': np.random.randint(0, 15, 9)})\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using pandas pivot table\ndf_grouped = pd.pivot_table(df[[\"fruit\", \"price\"]], index = [\"fruit\"], aggfunc = np.mean ).reset_index()\ndf_grouped\n\n# using groupby\nout = df.groupby('fruit', as_index=False)['price'].mean()\nout","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q70'></a>\n\n**70. How to join two dataframes by 2 columns so they have only the common rows?**\n\nJoin dataframes df1 and df2 by ‘fruit-pazham’ and ‘weight-kilo’.\n\n[Go back to the table of contents](#table_of_contents)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\ndf1 = pd.DataFrame({'fruit': ['apple', 'banana', 'orange'] * 3,\n                    'weight': ['high', 'medium', 'low'] * 3,\n                    'price': np.random.randint(0, 15, 9)})\n\ndf2 = pd.DataFrame({'pazham': ['apple', 'orange', 'pine'] * 2,\n                    'kilo': ['high', 'low'] * 3,\n                    'price': np.random.randint(0, 15, 6)})\ndf1\ndf2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Solution 1\n# using pandas merge\nmerge_df = pd.merge(df1, df2, left_on=[\"fruit\", \"weight\"], right_on=[\"pazham\", \"kilo\"])\nmerge_df\n\n\n# Solution from the webpage\npd.merge(df1, df2, how='inner', left_on=['fruit', 'weight'], right_on=['pazham', 'kilo'], suffixes=['_left', '_right'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q71'></a>\n\n**71. How to remove rows from a dataframe that are present in another dataframe?**\n\nFrom df1, remove the rows that are present in df2. All three columns must be the same.\n\n[Go back to the table of contents](#table_of_contents)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\ndf1 = pd.DataFrame({'fruit': ['apple', 'banana', 'orange'] * 3,\n                    'weight': ['high', 'medium', 'low'] * 3,\n                    'price': np.random.randint(0, 10, 9)})\n\ndf2 = pd.DataFrame({'pazham': ['apple', 'orange', 'pine'] * 2,\n                    'kilo': ['high', 'low'] * 3,\n                    'price': np.random.randint(0, 10, 6)})\n\ndf1\ndf2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We might use pandas merge\n#df1.merge(df2, how = \"inner\", left_on = [\"fruit\", \"weight\", \"price\"], right_on = [\"pazham\", \"kilo\", \"price\"])\n\ndf1[\"concat\"] = df1[\"fruit\"].astype(str) + df1[\"weight\"].astype(str) + df1[\"price\"].astype(str)\n#df1\n\ndf2[\"concat\"] = df2[\"pazham\"].astype(str) + df2[\"kilo\"].astype(str) + df2[\"price\"].astype(str)\n#df2\n\ndf1 = df1[~df1[\"concat\"].isin(df2[\"concat\"])]\ndf1.drop(\"concat\", inplace = True, axis = 1)\ndf1\n\n# Solution from the webpage, IMHO it's incorrect\n#df1[~df1.isin(df2).all(1)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q72'></a>\n\n**72. How to get the positions where values of two columns match?**\n\nFind the index where col fruit1 and fruit2 match\n\n[Go back to the table of contents](#table_of_contents)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\ndf = pd.DataFrame({'fruit1': np.random.choice(['apple', 'orange', 'banana'], 10),\n                    'fruit2': np.random.choice(['apple', 'orange', 'banana'], 10)})\ndf\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Solution\nnp.where(df.fruit1 == df.fruit2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q73'></a>\n\n**73. How to create lags and leads of a column in a dataframe?**\n\nCreate two new columns in df, one of which is a lag1 (shift column a down by 1 row) of column ‘a’ and the other is a lead1 (shift column b up by 1 row).\n\n[Go back to the table of contents](#table_of_contents)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\ndf = pd.DataFrame(np.random.randint(1, 100, 20).reshape(-1, 4), columns = list('abcd'))\ndf\n\n'''\nDesired Output\n\n    a   b   c   d  a_lag1  b_lead1\n0  66  34  76  47     NaN     86.0\n1  20  86  10  81    66.0     73.0\n2  75  73  51  28    20.0      1.0\n3   1   1   9  83    75.0     47.0\n4  30  47  67   4     1.0      NaN\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"lag1\"] = df[\"a\"].shift(1)\ndf[\"lead1\"] = df[\"b\"].shift(-1)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q74'></a>\n\n**74. How to get the frequency of unique values in the entire dataframe?**\n\nGet the frequency of unique values in the entire dataframe df.\n\n[Go back to the table of contents](#table_of_contents)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\ndf = pd.DataFrame(np.random.randint(1, 10, 20).reshape(-1, 4), columns = list('abcd'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Solution\npd.value_counts(df.values.ravel())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = 'q75'></a>\n\n**75. How to split a text column into two separate columns?**\n\nSplit the string column in df to form a dataframe with 3 columns as shown.\n\n[Go back to the table of contents](#table_of_contents)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input\ndf = pd.DataFrame([\"STD, City    State\",\n\"33, Kolkata    West Bengal\",\n\"44, Chennai    Tamil Nadu\",\n\"40, Hyderabad    Telengana\",\n\"80, Bangalore    Karnataka\"], columns=['row'])\n\ndf\n\n'''\nDesired Output\n\n0 STD        City        State\n1  33     Kolkata  West Bengal\n2  44     Chennai   Tamil Nadu\n3  40   Hyderabad    Telengana\n4  80   Bangalore    Karnataka\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we do \" \".join(x.split()) to replace multiple spaces to 1 space\n# we do split(None, 2, ) to split a string on the second space ()this way we have West Bengal together\ndf[\"re\"] = df[\"row\"].apply(lambda x: \" \".join(x.split()).split(None, 2, ))\n\nnew_header = df[\"re\"][0]\nvalues = df[\"re\"][1:]\n\n# our values is a series of lists, we have to do some list comprehension no extract the values\nd = {new_header[0]:[int(values.iloc[i][0].replace(\",\", \"\")) for i in range(len(values))], \\\n     new_header[1]:[values.iloc[i][1].replace(\",\", \"\") for i in range(len(values))], \\\n     new_header[2]:[values.iloc[i][2].replace(\",\", \"\") for i in range(len(values))]}\n\n# create a pandas DF from a dict\nnew_df = pd.DataFrame(d)\nnew_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The End\n# Thank you very much!"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}